{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References**\n",
    "- [https://spacy.pythonhumanities.com/intro.html](https://spacy.pythonhumanities.com/intro.html)  \n",
    "- [https://spacy.io/usage/spacy-101](https://spacy.io/usage/spacy-101)\n",
    "- [https://www.youtube.com/watch?v=dIUTsFT2MeQ&t=14s](https://www.youtube.com/watch?v=dIUTsFT2MeQ&t=14s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linguistic Annotations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/01_02_linguistic_annotations.html](https://spacy.pythonhumanities.com/01_02_linguistic_annotations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open ('wiki_us.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those text print look similar, but why the length difference. Let see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text[0:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count every single char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count individuals token (word, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text.split()[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (U.S.A. look kinda wrong, the punctuation should be saparate then we can remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent1 = doc.sents[0] # can not use\n",
    "\n",
    "sent1 = list(doc.sents)[0]\n",
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = sent1[2]\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.io/api/token](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.left_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.right_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.io/api/entityrecognizer](https://spacy.io/api/entityrecognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.ent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.ent_iob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1[12].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1[12].morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.pos_ # proper noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.dep_ # noun subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2.lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Mike enjoys playing football.'\n",
    "doc2 = nlp(text)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc2, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/01_03_word_vectors.html](https://spacy.pythonhumanities.com/01_03_word_vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.io/api/vocab](https://spacy.io/api/vocab)  \n",
    "[https://numpy.org/doc/stable/reference/generated/numpy.asarray.html](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_word = 'country'\n",
    "\n",
    "ms = nlp1.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp1.vocab.vectors[nlp1.vocab.strings[your_word]]]), n=10)\n",
    "words = [nlp1.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "\n",
    "print(nlp1.vocab.strings[your_word])\n",
    "print(ms)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.io/usage/linguistic-features](https://spacy.io/usage/linguistic-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp1('I like salty fries and hamburgers.')\n",
    "doc2 = nlp1('Fast food tastes very good.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc1, '<->', doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp1('The Empire State Building is in New York.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (doc1, '<->', doc3, doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect document overlap sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp1('I enjoy oranges.')\n",
    "doc5 = nlp1('I enjoy apples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc4, '<->', doc5, doc4.similarity(doc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = nlp1('I enjoy burgers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc4, '<->', doc6, doc4.similarity(doc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, '<->', burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/01_04_pipelines.html](https://spacy.pythonhumanities.com/01_04_pipelines.html)  \n",
    "[https://spacy.io/usage/processing-pipelines](https://spacy.io/usage/processing-pipelines)  \n",
    "[https://spacy.io/models](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.io/api/top-level](https://spacy.io/api/top-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://pypi.org/project/requests/](https://pypi.org/project/requests/)  \n",
    "[https://pypi.org/project/beautifulsoup4/](https://pypi.org/project/beautifulsoup4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !py -m pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\n",
    "soup = BeautifulSoup(s.content).text.replace('-\\n', '').replace('\\n', ' ')\n",
    "nlp2.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# doc_test = nlp2(soup)\n",
    "# print(len(list(doc.sents)))\n",
    "\n",
    "# Wall time: 19.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# nlp.max_length = 5278439\n",
    "# doc_test = nlp(soup)\n",
    "# print(len(list(doc.sents)))\n",
    "\n",
    "# Wall time: 3min 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EntityRuler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/02_01_entityruler.html](https://spacy.pythonhumanities.com/02_01_entityruler.html)  \n",
    "[https://spacy.io/api/entityruler](https://spacy.io/api/entityruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'West Chestertenfieldville was referenced in Mr. Deeds.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_e = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc_e.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The West Chestertenfieldville is GPE look correct. Now we want the Mr. Deeds is a film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe('entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example to change entity type of specific entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    { 'label': 'PERSON', 'pattern': 'West Chestertenfieldville' }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)\n",
    "\n",
    "for ent in doc_e.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not affected, cuz the **ner** step is before the **entity_ruler**, when the **ner** step to identify the \"West Chestertenfieldville\", the **entity_ruler** will not identify it again. To apply our rule, we need to move the **entity_ruler** before **ner** step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_e = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp_e.add_pipe('entity_ruler', before='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_e = nlp_e(text)\n",
    "\n",
    "for ent in doc_e.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_e.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_e1 = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp_e1.add_pipe('entity_ruler', before='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    { 'label': 'PERSON', 'pattern': 'West Chestertenfieldville' },\n",
    "    { 'label': 'FILM', 'pattern': 'Mr. Deeds' }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_e = nlp_e1(text)\n",
    "\n",
    "for ent in doc_e.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mr. Deeds is now detect as a film, but it could be a person. It called **toponym resolution** (common problem of nlp, thing with multiple label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Matcher**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/02_02_matcher.html](https://spacy.pythonhumanities.com/02_02_matcher.html)  \n",
    "[https://spacy.io/api/matcher](https://spacy.io/api/matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    { 'LIKE_EMAIL': True }\n",
    "]\n",
    "matcher.add('EMAIL_ADDRESS', [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_m = nlp('This is an email address: nghoangtan96@gmail.com')\n",
    "matchers = matcher(doc_m)\n",
    "print(matchers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.vocab[matchers[0][0]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('wiki_mlk.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_m = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Part-of-Speech\n",
    "print(spacy.explain('PROPN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{ 'POS': 'PROPN' }]\n",
    "matcher.add('PROPER_NOUN', [pattern])\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look not correct cuz the Martin Luther King Jr. should be a proper noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{ 'POS': 'PROPN', 'OP': '+' }]\n",
    "matcher.add('PROPER_NOUN', [pattern])\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{ 'POS': 'PROPN', 'OP': '+' }]\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy='LONGEST')\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look correct, but the order was messup, let check the bottom of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches[-10:]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{ 'POS': 'PROPN', 'OP': '+' }]\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy='LONGEST')\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "# (id, start_index, end_index)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we to know anytime proper noun follow by a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{ 'POS': 'PROPN', 'OP': '+' }, { 'POS': 'VERB' }]\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy='LONGEST')\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "# (id, start_index, end_index)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('alice.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# with open('alice.txt') as f:\n",
    "#     data = f.read()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['data'][0]['contents']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[0].replace('`', \"'\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know any speak inside the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" }\n",
    "]\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy='LONGEST')\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "# (id, start_index, end_index)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the speak, but we also want to know who is the speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_lemmas = ['think', 'say']\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" },\n",
    "\n",
    "    { 'POS': 'VERB', 'LEMMA': { 'IN': speak_lemmas }}, # select a verb follow which is match speak_lemmas (think > thought > thought)\n",
    "    { 'POS': 'PROPN', 'OP': '+' },\n",
    "\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" }\n",
    "]\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy='LONGEST')\n",
    "doc_m = nlp(text)\n",
    "matches = matcher(doc_m)\n",
    "# (id, start_index, end_index)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in data:\n",
    "    text = text.replace('`', \"'\")\n",
    "    doc_m = nlp(text)\n",
    "    matches = matcher(doc_m)\n",
    "    print(len(matches))\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pattern can not capture any match of other content cuz now we specific capture <```string in quote```><```verb + proper noun```><```string in quote```> and only 1 matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher1 = Matcher(nlp_m.vocab)\n",
    "pattern1 = [\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" },\n",
    "\n",
    "    { 'POS': 'VERB', 'LEMMA': { 'IN': speak_lemmas }}, # select a verb follow which is match speak_lemmas (think > thought > thought)\n",
    "    { 'POS': 'PROPN', 'OP': '+' },\n",
    "\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" }\n",
    "]\n",
    "pattern2 = [\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" },\n",
    "\n",
    "    { 'POS': 'VERB', 'LEMMA': { 'IN': speak_lemmas }}, # select a verb follow which is match speak_lemmas (think > thought > thought)\n",
    "    { 'POS': 'PROPN', 'OP': '+' }\n",
    "]\n",
    "pattern3 = [\n",
    "    { 'POS': 'VERB', 'LEMMA': { 'IN': speak_lemmas }}, # select a verb follow which is match speak_lemmas (think > thought > thought)\n",
    "    { 'POS': 'PROPN', 'OP': '+' },\n",
    "\n",
    "    { 'ORTH': \"'\" },\n",
    "    { 'IS_ALPHA': True, 'OP': '+' }, # to select all text inside quote\n",
    "    { 'IS_PUNCT': True, 'OP': '*' }, # to select any or non punc inside quote\n",
    "    { 'ORTH': \"'\" }\n",
    "]\n",
    "matcher.add('PROPER_NOUNS', [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
    "\n",
    "for text in data:\n",
    "    text = text.replace('`', \"'\")\n",
    "    doc_m = nlp(text)\n",
    "    matches = matcher(doc_m)\n",
    "    print(len(matches))\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc_m[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom Components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/02_04_custom_component.html](https://spacy.pythonhumanities.com/02_04_custom_component.html)  \n",
    "[https://spacy.io/usage/processing-pipelines#custom-components](https://spacy.io/usage/processing-pipelines#custom-components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cc = spacy.load('en_core_web_sm')\n",
    "doc_cc = nlp_cc('Britain is a place. Mary is a doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc_cc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component('remove_gpe')\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cc.add_pipe('remove_gpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cc.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cc = nlp_cc('Britain is a place. Mary is a doctor')\n",
    "for ent in doc_cc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cc.to_disk('new_en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RegEx**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/02_05_simple_regex.html](https://spacy.pythonhumanities.com/02_05_simple_regex.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample text\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \n",
    "                    \"pattern\": [\n",
    "                        {\"TEXT\": {\"REGEX\": r\"((\\d){3}-(\\d){4})\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can not use to match across tokens. The dash in the phone number throws off the EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample number 5555555.\"\n",
    "\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \n",
    "                    \"pattern\": [\n",
    "                        {\"TEXT\": {\"REGEX\": r\"((\\d){5})\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RegEx (Multi-Word Tokens)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Paul [A-Z]\\w+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rg = spacy.blank('en')\n",
    "doc_rg = nlp_rg(text)\n",
    "doc_rg.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ents = list(doc_rg.ents)\n",
    "mwt_ents = []\n",
    "\n",
    "for match in re.finditer(pattern, doc_rg.text):\n",
    "    start, end = match.span()\n",
    "    span = doc_rg.char_span(start, end)\n",
    "    print(span)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mwt_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in mwt_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc_rg, start, end, label='PERSON')\n",
    "    original_ents.append(per_ent)\n",
    "doc_rg.ents = original_ents\n",
    "print(doc_rg.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component('paul_ner')\n",
    "def paul_ner(doc):\n",
    "    pattern = r'Paul [A-Z]\\w+'\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        print(span)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label='PERSON')\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rg1 = spacy.blank('en')\n",
    "nlp_rg1.add_pipe('paul_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_rg1 = nlp_rg1(text)\n",
    "print(doc_rg1.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let create a new Span to detect Holywood as CINEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component('cinema_ner')\n",
    "def paul_ner(doc):\n",
    "    pattern = r'Hollywood'\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        print(span)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label='CINEMA')\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rg2 = spacy.load('en_core_web_sm')\n",
    "nlp_rg2.add_pipe('cinema_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_rg2 = nlp_rg2(text)\n",
    "# [E1010] Unable to set entity information for token 9 which is \n",
    "# included in more than one span in entities, blocked, missing or outside.\n",
    "\n",
    "# The error because the overlap span of cinema_ner and paul_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "@Language.component('cinema_ner1')\n",
    "def paul_ner(doc):\n",
    "    pattern = r'Hollywood'\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        print(span)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label='CINEMA')\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The priority given to longer token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_rg3 = spacy.load('en_core_web_sm')\n",
    "nlp_rg3.add_pipe('cinema_ner1')\n",
    "doc_rg3 = nlp_rg3(text)\n",
    "\n",
    "for ent in doc_rg3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applied SpaCy Financi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://spacy.pythonhumanities.com/03_01_stock_analysis.html](https://spacy.pythonhumanities.com/03_01_stock_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stocks.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = df.Symbol.tolist()\n",
    "companies = df.CompanyName.tolist()\n",
    "print(symbols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'STOCK', 'pattern': 'A'}, {'label': 'STOCK', 'pattern': 'AA'}, {'label': 'STOCK', 'pattern': 'AAC'}, {'label': 'STOCK', 'pattern': 'AACG'}, {'label': 'STOCK', 'pattern': 'AADI'}, {'label': 'STOCK', 'pattern': 'AAIC'}, {'label': 'STOCK', 'pattern': 'AAL'}, {'label': 'STOCK', 'pattern': 'AAMC'}, {'label': 'STOCK', 'pattern': 'AAME'}, {'label': 'STOCK', 'pattern': 'AAN'}]\n",
      "[{'label': 'COMPANY', 'pattern': 'Zoetis'}, {'label': 'COMPANY', 'pattern': 'Zumiez'}, {'label': 'COMPANY', 'pattern': 'Zuora'}, {'label': 'COMPANY', 'pattern': 'Zevia'}, {'label': 'COMPANY', 'pattern': 'Zovio'}, {'label': 'COMPANY', 'pattern': 'Z-Work Acquisition'}, {'label': 'COMPANY', 'pattern': 'Zymergen'}, {'label': 'COMPANY', 'pattern': 'Zymeworks'}, {'label': 'COMPANY', 'pattern': 'Zynerba Pharmaceuticals'}, {'label': 'COMPANY', 'pattern': 'Zynex'}]\n"
     ]
    }
   ],
   "source": [
    "stops = ['two']\n",
    "nlp_asf = spacy.blank('en')\n",
    "ruler = nlp_asf.add_pipe('entity_ruler')\n",
    "patterns = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    patterns.append({ 'label': 'STOCK', 'pattern': symbol })\n",
    "\n",
    "for company in companies:\n",
    "    if company not in stops:\n",
    "        patterns.append({ 'label': 'COMPANY', 'pattern': company })\n",
    "\n",
    "print(patterns[:10])\n",
    "print(patterns[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Sept 10 (Reuters) - Wall Street's main indexes were subdued on Friday as signs of higher inflation and a drop in Apple shares following an unfavorable court ruling offset expectations of an easing in U.S.-China tensions.\n",
    "\n",
    "Data earlier in the day showed U.S. producer prices rose solidly in August, leading to the biggest annual gain in nearly 11 years and indicating that high inflation was likely to persist as the pandemic pressures supply chains. read more .\n",
    "\n",
    "\"Today's data on wholesale prices should be eye-opening for the Federal Reserve, as inflation pressures still don't appear to be easing and will likely continue to be felt by the consumer in the coming months,\" said Charlie Ripley, senior investment strategist for Allianz Investment Management.\n",
    "\n",
    "Apple Inc (AAPL.O) fell 2.7% following a U.S. court ruling in \"Fortnite\" creator Epic Games' antitrust lawsuit that stroke down some of the iPhone maker's restrictions on how developers can collect payments in apps.\n",
    "\n",
    "\n",
    "Sponsored by Advertising Partner\n",
    "Sponsored Video\n",
    "Watch to learn more\n",
    "Report ad\n",
    "Apple shares were set for their worst single-day fall since May this year, weighing on the Nasdaq (.IXIC) and the S&P 500 technology sub-index (.SPLRCT), which fell 0.1%.\n",
    "\n",
    "Sentiment also took a hit from Cleveland Federal Reserve Bank President Loretta Mester's comments that she would still like the central bank to begin tapering asset purchases this year despite the weak August jobs report. read more\n",
    "\n",
    "Investors have paid keen attention to the labor market and data hinting towards higher inflation recently for hints on a timeline for the Federal Reserve to begin tapering its massive bond-buying program.\n",
    "\n",
    "The S&P 500 has risen around 19% so far this year on support from dovish central bank policies and re-opening optimism, but concerns over rising coronavirus infections and accelerating inflation have lately stalled its advance.\n",
    "\n",
    "\n",
    "Report ad\n",
    "The three main U.S. indexes got some support on Friday from news of a phone call between U.S. President Joe Biden and Chinese leader Xi Jinping that was taken as a positive sign which could bring a thaw in ties between the world's two most important trading partners.\n",
    "\n",
    "At 1:01 p.m. ET, the Dow Jones Industrial Average (.DJI) was up 12.24 points, or 0.04%, at 34,891.62, the S&P 500 (.SPX) was up 2.83 points, or 0.06%, at 4,496.11, and the Nasdaq Composite (.IXIC) was up 12.85 points, or 0.08%, at 15,261.11.\n",
    "\n",
    "Six of the eleven S&P 500 sub-indexes gained, with energy (.SPNY), materials (.SPLRCM) and consumer discretionary stocks (.SPLRCD) rising the most.\n",
    "\n",
    "U.S.-listed Chinese e-commerce companies Alibaba and JD.com , music streaming company Tencent Music (TME.N) and electric car maker Nio Inc (NIO.N) all gained between 0.7% and 1.4%\n",
    "\n",
    "\n",
    "Report ad\n",
    "Grocer Kroger Co (KR.N) dropped 7.1% after it said global supply chain disruptions, freight costs, discounts and wastage would hit its profit margins.\n",
    "\n",
    "Advancing issues outnumbered decliners by a 1.12-to-1 ratio on the NYSE and by a 1.02-to-1 ratio on the Nasdaq.\n",
    "\n",
    "The S&P index recorded 14 new 52-week highs and three new lows, while the Nasdaq recorded 49 new highs and 38 new lows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "Nasdaq COMPANY\n",
      "ET STOCK\n",
      "Nasdaq COMPANY\n",
      "JD.com COMPANY\n",
      "Kroger COMPANY\n",
      "Nasdaq COMPANY\n",
      "Nasdaq COMPANY\n"
     ]
    }
   ],
   "source": [
    "doc_asf = nlp_asf(text)\n",
    "for ent in doc_asf.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc_asf, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We missing AAPL at second block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "for symbol in symbols:\n",
    "    for l in letters:\n",
    "        patterns.append({\"label\": \"STOCK\", \"pattern\": symbol+f\".{l}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)\n",
    "doc_asf = nlp_asf(text)\n",
    "displacy.render(doc_asf, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "AAPL.O STOCK\n",
      "Apple COMPANY\n",
      "Nasdaq COMPANY\n",
      "two COMPANY\n",
      "ET STOCK\n",
      "Nasdaq COMPANY\n",
      "JD.com COMPANY\n",
      "TME.N STOCK\n",
      "NIO.N STOCK\n",
      "Kroger COMPANY\n",
      "KR.N STOCK\n",
      "Nasdaq COMPANY\n",
      "Nasdaq COMPANY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc_asf.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IndexName</th>\n",
       "      <th>IndexSymbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dow Jones Industrial Average</td>\n",
       "      <td>DJIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dow Jones Transportation Average</td>\n",
       "      <td>DJT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dow Jones Utility Average Index</td>\n",
       "      <td>DJU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NASDAQ 100 Index (NASDAQ Calculation)</td>\n",
       "      <td>NDX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NASDAQ Composite Index</td>\n",
       "      <td>COMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               IndexName IndexSymbol\n",
       "0           Dow Jones Industrial Average        DJIA\n",
       "1       Dow Jones Transportation Average         DJT\n",
       "2        Dow Jones Utility Average Index         DJU\n",
       "3  NASDAQ 100 Index (NASDAQ Calculation)         NDX\n",
       "4                 NASDAQ Composite Index        COMP"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('indexes.tsv', sep='\\t')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df2.IndexName.tolist()\n",
    "index_symbols = df2.IndexSymbol.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    patterns.append({ 'label': 'INDEX', 'pattern': index })\n",
    "    words = index.split()\n",
    "    patterns.append({ 'label': 'INDEX', 'pattern': ' '.join(words[:2]) })\n",
    "for index in index_symbols:\n",
    "    patterns.append({ 'label': 'INDEX', 'pattern': index })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "Nasdaq COMPANY\n",
      "S&P 500 INDEX\n",
      "S&P 500 INDEX\n",
      "ET STOCK\n",
      "Dow Jones Industrial Average INDEX\n",
      "S&P 500 INDEX\n",
      "Nasdaq COMPANY\n",
      "S&P 500 INDEX\n",
      "JD.com COMPANY\n",
      "Kroger COMPANY\n",
      "Nasdaq COMPANY\n",
      "Nasdaq COMPANY\n"
     ]
    }
   ],
   "source": [
    "doc_asf = nlp_asf(text)\n",
    "for ent in doc_asf.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BloombergExchangeCode</th>\n",
       "      <th>BloombergCompositeCode</th>\n",
       "      <th>Country</th>\n",
       "      <th>Description</th>\n",
       "      <th>ISOMIC</th>\n",
       "      <th>Google Prefix</th>\n",
       "      <th>EODcode</th>\n",
       "      <th>NumStocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF</td>\n",
       "      <td>AR</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Bolsa de Comercio de Buenos Aires</td>\n",
       "      <td>XBUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AO</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>National Stock Exchange of Australia</td>\n",
       "      <td>XNEC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Asx - All Markets</td>\n",
       "      <td>XASX</td>\n",
       "      <td>ASX</td>\n",
       "      <td>AU</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Wiener Boerse Ag</td>\n",
       "      <td>XWBO</td>\n",
       "      <td>VIE</td>\n",
       "      <td>VI</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>Bahrain Bourse</td>\n",
       "      <td>XBAH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BloombergExchangeCode BloombergCompositeCode    Country  \\\n",
       "0                    AF                     AR  Argentina   \n",
       "1                    AO                     AU  Australia   \n",
       "2                    AT                     AU  Australia   \n",
       "3                    AV                    NaN    Austria   \n",
       "4                    BI                    NaN    Bahrain   \n",
       "\n",
       "                            Description ISOMIC Google Prefix EODcode NumStocks  \n",
       "0     Bolsa de Comercio de Buenos Aires   XBUE           NaN      BA        12  \n",
       "1  National Stock Exchange of Australia   XNEC           NaN     NaN         1  \n",
       "2                     Asx - All Markets   XASX           ASX      AU       875  \n",
       "3                      Wiener Boerse Ag   XWBO           VIE      VI        38  \n",
       "4                        Bahrain Bourse   XBAH           NaN     NaN         4  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('stock_exchanges.tsv', sep='\\t')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XBUE', 'XNEC', 'XASX', 'XWBO', 'XBAH', 'XDHA', 'XBRU', 'BVMF', 'XCNQ', 'XTSE']\n"
     ]
    }
   ],
   "source": [
    "exchanges = df3.ISOMIC.tolist() + df3['Google Prefix'].tolist() + df3.Description.tolist()\n",
    "print(exchanges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in exchanges:\n",
    "    patterns.append({ 'label': 'STOCK_EXCHANGE', 'pattern': e })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "Nasdaq COMPANY\n",
      "S&P 500 INDEX\n",
      "S&P 500 INDEX\n",
      "ET STOCK\n",
      "Dow Jones Industrial Average INDEX\n",
      "S&P 500 INDEX\n",
      "Nasdaq COMPANY\n",
      "S&P 500 INDEX\n",
      "JD.com COMPANY\n",
      "Kroger COMPANY\n",
      "NYSE STOCK_EXCHANGE\n",
      "Nasdaq COMPANY\n",
      "Nasdaq COMPANY\n"
     ]
    }
   ],
   "source": [
    "ruler.add_patterns(patterns)\n",
    "doc_asf = nlp_asf(text)\n",
    "for ent in doc_asf.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
